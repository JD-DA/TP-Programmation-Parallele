{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 MPI et les communications collectives\n",
    "## M1 informatique, Université d'Orléans 2021/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est de mettre en place des programmes encore simples avec les routines de communications collectives. \n",
    "Vous allez également pouvoir exécuter vos codes sur une machine parallèle afin de faire quelques tests de performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Le calcul de $\\pi$\n",
    "La valeur de $\\pi$ peut être calculée par l'intégrale $\\int_{0}^{1} \\frac{4}{1+x^2}$. Cette intégrale peut être approchée par $\\pi \\approx \\sum_{i=0}^nf(x_i)\\Delta x$ où $\\Delta x=\\frac{1}{n}$ et $x_i=\\frac{1}{2n}+i\\frac{1}{n}$\n",
    "\n",
    "Plus $n$ est grand plus on se rapproche de $\\pi$. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q1. Proposez une implémentation MPI qui permet de répartir le calcul de la somme sur différents processus d'une exécution parallèle. Le résultat final sera disponible sur le processus 'root' donné en ligne de commande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mpi.h>\n",
    "#include <iostream>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "double fx(double x)\n",
    "{\n",
    "  double res =(double)4.0/(1+x*x);\n",
    "  return res;\n",
    "}\n",
    "\n",
    "\n",
    "int main ( int argc , char **argv )\n",
    "{\n",
    "  int pid, nprocs;  \n",
    "  MPI_Init (&argc , &argv) ;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid ) ;\n",
    "  MPI_Comm_size (MPI_COMM_WORLD, &nprocs ) ;\n",
    "  \n",
    "  int n = atoi(argv[1]);\n",
    "  int root = atoi(argv[2]);  \n",
    "  \n",
    "  double Pi;  \n",
    " \n",
    "  // A compléter   \n",
    "    \n",
    "  if (pid==root) {\n",
    "    cout << \"PI=\" << setprecision (15) << Pi << endl;\n",
    "  }\n",
    "  MPI_Finalize() ;\n",
    "  return 0 ;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalisation d'un vecteur\n",
    "\n",
    "Soit *V* un vecteur de taille *n* généré sur un processus *root*. On souhaite calculer la normalisation de ce vecteur en partageant le travail sur les différents processus et en rassemblant le résultat sur le processus *root*.\n",
    "\n",
    "$$\n",
    "V_{\\operatorname{norm}} = \\frac{1}{\\left\\|V\\right\\|} V \\quad\\mbox{avec}\\quad\n",
    "\\left\\|V\\right\\| = \\sqrt{\\sum_{i=0}^{n-1} V_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q2. En supposant que n est divisible par le nombre de processus de l'exécution parallèle, écrivez un programme MPI dont le résultat est le vecteur V normalisé et rassemblé sur le processeur root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Modifiez la version précédente pour lever la condition n divisible par le nombre de processus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exécution sur ptimirev\n",
    "\n",
    "La machine *ptimirev* est une grappe de PC reliée par un réseau Ethernet. Elle est constituée de 5 nœuds dont ptimirev-server qui est le point d'entrée à partir de votre compte des salles machines (adresse IP de ptimirev-server 192.168.80.201). Ensuite vous pouvez accéder aux 4 autres nœuds ptimirev1, ptimirev2, ptimirev3 et ptimirev4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.0 __Préliminaires__\n",
    "    1. ssh o'num étudiant'@ptimirev-server (mot de passe code NNE)\n",
    "    2. ssh-keygen (tout valider sans rentrer de mot de passe)\n",
    "    3. cat .ssh/id_rsa.pub >> .ssh/authorized_keys (pour ajouter la clé publique aux autorisations)\n",
    "    4. for i in 1 2 3 4; do ssh ptimirev$i echo Ok; done (valider l'identité des 4 machines)\n",
    "    5. félicitations ! vous pouvez désormais vous connecter sans mot de passe aux 4 nœuds\n",
    "    \n",
    "Si vous préférez utiliser une clé SSH avec mot de passe, alors il est nécessaire avant de lancer MPI d'activer un agent d'authentification SSH et de lui fournir la clé privée. Cela peut se faire, par exemple, en une commande qu'il faudra utiliser à **chaque session** : eval $(ssh-agent); ssh-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 __Gestion de l'exécution parallèle__\n",
    "\n",
    "Lorsqu'on exécute le programme sur une machine à mémoire distribuée, **mpirun** gère le lancement du programme sur chaque processus à distance par ssh. Il faut donc que l'exécutable soit accessible via le **PATH**. Pour cela vous pouvez rajouter au fichier **.bashrc** les lignes suivantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ -d \"$HOME/bin\" ] ; then\n",
    "    export PATH=\"$HOME/bin:$PATH\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi vous pourrez placer votre exécutable dans ce répertoire bin de votre $HOME et lors de l'exécution parallèle le lancement à distance par mpirun pourra fonctionner.\n",
    "\n",
    "Enfin, avant de pouvoir exécuter le programme il faut définir les processus que l'on souhaite utiliser. Pour cela il faut créer un fichier qui va contenir ces informations. Voici un exemple (le fichier créé se nomme liste_machines)\n",
    "\n",
    "ptimirev1 slots=2<br/>\n",
    "ptimirev2 slots=2<br/>\n",
    "ptimirev3 slots=2<br/>\n",
    "ptimirev4 slots=2<br/>\n",
    "\n",
    "La syntaxe pour utiliser ce fichier et lancer le programme sur les processus correspondants est\n",
    "\n",
    "mpirun --hostfile liste_machines (-np 4) 'NomExecutable' 'les arguments'\n",
    "\n",
    "Si vous précisez le nombre de processus avec l'option -np x le programme s'exécutera sur x processus choisis dans le fichier listes_machines. Sans cette option il utilise toutes les machines données dans le fichier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 __Performances de la normalisation d'un vecteur__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q4. Testez la performance de votre implémentation de la normalisation d'un vecteur. Pour cela calculez l'accélération et l'efficacité de votre programme pour différentes tailles du vecteur et pour 1, 2, 4 et 8 processus.\n",
    "Le temps peut être calculé par le processus 'root' entre l'initialisation du vecteur et l'écriture du résultat."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q5. Comparez les performances avec la version du TP précédent n'utilisant pas de routines de communications collectives."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q6. Testez différentes versions du fichier 'liste_machines' en fonction du nombre de 'slots' sur chaque noeud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <random>\n",
    "#include <chrono>\n",
    "#include <mpi.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    int pid, nprocs;\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n",
    "\n",
    "    chrono::time_point<chrono::system_clock> start, end;\n",
    "    \n",
    "    int n = atoi(argv[1]); // la taille du tableau global\n",
    "    int root = atoi(argv[2]); // le processeur root\n",
    "    \n",
    "    double *vecteur;\n",
    "\n",
    "    if (pid == root) {\n",
    "      vecteur = new double[n];\n",
    "      random_device rd;\n",
    "      mt19937 gen(rd());\n",
    "      uniform_real_distribution<> dis(0, 10.0);\n",
    "      for (int i = 0; i < n; i++)\n",
    "        vecteur[i] = dis(gen);\n",
    "      \n",
    "      cout << \"le tableau initial est \";\n",
    "      for (int i = 0; i < n; i++)\n",
    "        cout << vecteur[i] << \" \";\n",
    "      cout << endl;\n",
    "    }\n",
    "\n",
    "    if (pid==root) {\n",
    "      start = chrono::system_clock::now();\n",
    "    }\n",
    "\n",
    "    // Votre code ici\n",
    "    \n",
    "    if (pid==root) { //Calcul du temps écoulé.\n",
    "      end = chrono::system_clock::now();\n",
    "      chrono::duration<double> elapsed_seconds = end-start;\n",
    "      cout << \"tps=\" << elapsed_seconds.count() << endl;\n",
    "    }\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Suite monotone\n",
    "\n",
    "Une suite est monotone si elle est strictement croissante ou strictement décroissante. On souhaite définir si un tableau de réels $U$ de taille $n$ correspond à une suite monotone. Le processus *root* est le seul à avoir initialement le tableau $U$.\n",
    "\n",
    "Dans une première étape le tableau $U$ est distribué avec un *MPI_Scatterv*. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q7. Quel est le principe de la parallélisation ?\n",
    "Q8. Est ce suffisant si chaque processus vérifie si les données reçues forment une suite monotone ? Quelles communications faut-il rajouter ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q9. Ecrivez le programme correspondant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
